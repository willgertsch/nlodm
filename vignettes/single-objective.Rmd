---
title: "Finding Single Objective Designs"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Finding Single Objective Designs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.asp = 0.8,
  fig.width = 10,
  out.width = "100%",
  fig.align = "center"
  
)
```

```{r setup}
library(nlodm)
```

## Introduction

The `nlodm` package can find optimal designs for nonlinear dose response models of the form $$
P(d) = f(d; \theta) + \epsilon
$$ The package supports a large variety of different models and objectives.

## Optimal design theory

A design for a dose-response experiment is a set of $k$ doses, $d_1, \dots, d_k$ and the numbers of subjects at each dose, $n_1, \dots, n_k$. In practice it is easier to work with the weights $w_i = N/n_i$, where $N$ is the total sample size. We can then write the $k$ point design as a probability measure $$
\xi = \begin{pmatrix} d_1 &\dots &d_k\\w_1 &\dots& w_k\end{pmatrix}
$$ For a given design $\xi$ and model function $f(d;\theta)$, we can construct a matrix that is proportional to the observed information matrix if the model was estimated from the design.

$$
M = M(\xi, \theta) = \sum_{i=1}^k w_i M_i(\xi, \theta) = \sum_{i=1}^k w_i \nabla f(d_i, \theta) \nabla f^T(d_i, \theta)
$$ where $\nabla f(d_i, \theta)$ is the gradient of the model function at dose $d_i$.

Once the information matrix is constructed, we can compute an objective function to judge how optimal the candidate design is. A very popular choice of objective function is

$$
\Psi_D(M) = \text{logdet} (M)
$$

Maximizing $\Psi_D(M)$ will minimize the volume of the confidence region for the regression parameters.

Once we have the design, we can use a result by [Kiefer (1960)](https://www.jstor.org/stable/2958055) to determine if the design is optimal. The `nlodm` function will automatically generate a graphical check to see if the design is optimal.

## Finding optimal designs

We will demonstrate how to use the `nlodm` function to find the D-optimal design for the log-logistic model.

First, the function requires the gradient function of the model be supplied. This allows the code to find designs for custom models easily. There are several built-in gradient functions and we can look at the gradient function for the log-logistic model.

```{r}
grad.loglogistic
```

In short, the `nlodm` function expects the gradient function to take two arguments, `x`and `theta`, and return a vector of the gradient components. This is helpful if you want to define your own gradient functions. Alternatively, we can use the `model` argument to select from the list of built-in models.

The design criteria can be chosen by specifying the `obj` argument. Current options are `'D'`, `'A'`, and `'bmd'`.

The `theta` option is for specifying the local parameter values for the model. These values can be obtained from a prior experiment or a reasonable guess based on theory. The values in the example were obtained from an example dataset. Later, we will see how to provide multiple values for theta to find Bayesian designs.

The `pts` and `bound` options control the number of design points and the upper dose limit for the experiment. The package will automatically prune design points with low weight, but cannot add more points if the initial number is too small. Since the number of points in the optimal design tends to match the number of paramters, we set the number of design points equal to 3.

The `algorithm`, `swarm`, `iter`, and `seed` arguments control the metaheuristic algorithm. The algorithms are supplied by the `metaheuristicOpt` package and you can see what algorithms are available by running `?metaheuristicOpt::metaOpt`. We will use particle swarm optimization with a swarm size of 30 and 400 iterations to find the design

```{r, results='hide'}
out = nlodm(
    model = NULL,
    grad_fun = grad.loglogistic,
    obj = 'D',
    theta = c(0.02461, -2.390, 1),
    bound = 30,
    pts = 3,
    algorithm = 'PSO',
    swarm = 30,
    iter = 400,
    seed = 1234
  )
```

The `nlodm` function returns a list containing the design with some rounding and the equivalence theorem plot to check for optimality.

```{r}
out$design
```

```{r}
out$plot
```

The plot indicates the design is optimal among all designs on this dose range.

If you want to use the original unrounded doses and weights, we can access the raw output from the optimizer. This will also give access to how long the optimizer took to find the optimum.

```{r}
out$raw
```

## Design efficiency

It is useful in practice to see how a candidate design compares to the optimal design. This can be done using design efficiency. For D-optimality, D-efficiency is defined as

$$
 \operatorname{eff}_D(\xi) = \left(\frac{\text{det}(\Psi_D(M(\xi)))}{\text{det}(\Psi_D(M(\xi_D)))}\right)^{1/p}
$$

where $\xi_D$ is the D-optimal design and $p$ is the number of parameters in the model.

We can use the `compute_eff` function to compute the design efficiency. As an example, we compare the efficiency of a uniform design to the D-optimal design obtained in the last section.

```{r}
compute_eff(
  model = NULL,
  grad_fun = grad.loglogistic,
  objective = 'D',
  theta = c(0.02461,-2.390, 1),
  d1 = c(0.1, 15, 30),
  d2 = out$design$x,
  w1 = c(1 / 3, 1 / 3, 1 / 3),
  w2 = out$design$w
)
```

This result indicates that the uniform design will need twice the sample size to be as effective at estimating the parameters as the D-optimal design.

## Designs for estimating the benchmark dose

Estimating the benchmark dose (BMD) is the goal of many toxicology experiments. The BMD is the point on the dose response curve that corresponds to a specific increase in risk over the zero dose. See [Haber et. al. (2018)](https://doi.org/10.1080/10408444.2018.1430121) for a review of the BMD. In this section, we will show the basics of how to derive a dual criterion for estimating the BMD and show how to use the `nlmod` function to find the optimal design for estimating the BMD.

There are two ways to measure an increase in risk over the zero dose. Added risk is defined as

$$
\pi_A(d) = P(d) - P(0).
$$

Extra risk is defined as

$$
\pi_E(d) = \frac{P(d) - P(0)}{1 - P(0)}
$$

For this example, we will assume the logistic model and use the added risk definition. The logistic model is defined as

$$
P(d) = \frac{1}{1 + \exp(-\beta_0 - \beta_1 d)}
$$

The BMD for an added risk of $r$ is

$$
\operatorname{BMD}(r) = \frac{\log\left( \frac{1-r}{1+r \exp(-\beta_0)}\right)}{\beta_1}.
$$

To derive a design criterion to estimate $BMD(r)$, we can take the gradient with respect to the the regression parameters.

$$
c = \left(\frac{\partial \operatorname{BMD}(r)}{\partial \beta_0},  \frac{\partial \operatorname{BMD}(r)}{\partial \beta_1}\right)^T
$$

The numeric vector $c$ can be used to construct a c-optimality objective function

$$
\Psi_c(M) = c^T M^{-1}c
$$

In practice, this objective function is not very useful because it will place all the design points at the prior guess for the BMD. A more useful criterion combines the c criterion with the D-optimality criterion to produce a design that has the same number of points as the D-optimal design, but with modified doses and weights to better estimate the BMD.

$$
\Psi_{CD}^\lambda(M) = \left[ \text{eff}_D(M) \right]^{1-\lambda} \left[\text{eff}_c (M)\right]^\lambda
$$

Equivalently, we can minimize the convex function

$$
\Psi_{BMD}^\lambda(M) = \frac{(1-\lambda)}{p} \log (|M|) - \lambda \log \left(c' M^{-1} c \right)
$$

The parameter $\lambda$ controls the relative importance of each objective, A value of $\lambda$ close to 1 will prioritize the c objective while a value close to 0 will prioritize the D objective. It is recommended to try several values of $\lambda$ in order to find the largest value that will still give a high D-efficiency.

To demonstrate using `nlodm` to find BMD designs, we will find a design for a log-logistic model with an added risk of 0.1. We set $\lambda = 0.5$ as a starting place that can be fine tuned later.

```{r, results='hide'}
out_bmd = nlodm(
  model = 'Log-logistic',
  grad_fun = grad.loglogistic,
  obj = 'bmd',
  theta = c(0.02461,-2.390, 1),
  bound = 30,
  pts = 3,
  algorithm = 'DE',
  swarm = 50,
  iter = 500,
  seed = 1234,
  bmd_type = 'added',
  risk = 0.1,
  lambda = 0.5
)
```

```{r}
out_bmd$design
```

Note that this design is no longer equally weighted like the D-optimal design, but places more weight on the dose closest to the BMD.

```{r}
out_bmd$plot
```

We can compare this design to the D-optimal design to get the D-efficiency.

```{r}
compute_eff(
  model = NULL,
  grad_fun = grad.loglogistic,
  objective = 'D',
  theta = c(0.02461,-2.390, 1),
  d1 = out_bmd$design$x,
  d2 = out$design$x,
  w1 = out_bmd$design$w,
  w2 = out$design$w
)
```

## (pseudo) Bayesian designs

In the previous sections, the designs found depended on a prior value for the regression parameters $\beta$. This means that the designs are only optimal if value of $\beta$ is correct. This is unlikely to be the case in practice, since if $\beta$ was known, there would be no need to run the experiment. These designs that depend on singular values of $\beta$ are usually called locally optimal designs.

To address the issue of local optimality, we can assume a prior distribution for $\beta$ and average the objective function over this distribution. If we assume a discrete prior distribution for $\beta$, we get the objective function

$$
\Psi_B(M) = \sum_j \Psi(M(\xi; \beta_j)) p_j
$$

where $p_j$ is the prior weight of the jth value of $\beta$ and $\sum_j p_j = 1$. Since $\Psi_B(M)$ is a convex combination of convex functions, the optimality check still holds and we can use the usual graphical check.

Finding Bayesian designs is straightforward using the `nlodm` function. If `theta` is a matrix, with each row corresponding to 1 value of $\theta$, then the criterion will automatically be averaged over the values using the provided weights.

As an example, we will generate a random sample of $\theta$ from a multivariate normal prior and find the Bayesian D-optimal design.

```{r}
set.seed(1234)
N = 100
theta0 = rnorm(N, -1.710, .3)
theta1 = rnorm(N, 0.09703, .01)
```

It is difficult to get an idea of the underlying dose response curve by just looking at the parameter values, so we will plot the dose response functions generated from this prior. We will be using a logistic function for this design problem.

```{r}
step = 30/1000
xvals = seq(0, 30, step)
plot(1/(1 + exp(-mean(theta0) - mean(theta1)*xvals)) ~ xvals, col = 'red')
for (i in 1:N)
  lines(1/(1 + exp(-theta0[i] - theta1[i]*xvals)) ~ xvals, lty = 2)
```

Now find the Bayesian D-optimal design assuming that all the prior values are equally likely.

```{r, results='hide'}
thetam = cbind(theta0, theta1)
prior_weights = rep(1 / N, N)

out_bayes = nlodm(
  model = NULL,
  grad_fun = grad.logistic,
  obj = 'D',
  theta = thetam,
  prior_weights = prior_weights,
  bound = 30,
  pts = 4,
  algorithm = 'PSO',
  swarm = 30,
  iter = 200,
  seed = 1234
)
```

```{r}
out_bayes$design
```

```{r}
out_bayes$plot
```

## Example 2: designs for the compartmental model

This is example 17.4 from Atkinson (Optimum experimental Designs, with SAS).

The compartmental model is defined as

$$
\eta(t, \theta) = \theta_3 \left\{ \exp(-\theta_2 t) - \exp(-\theta_1t)\right\}
$$

for $t \geq 0$, $\theta_1 > 0$, $\theta_2 > 0$, $\theta_3 >0$, and $\theta_1 > \theta_2$. This is a special case of a class of models commonly used in pharmacokinetics. Taking partial derivatives, we get the gradient components

$$
\frac{\partial\eta(t, \theta)}{\partial \theta_1} = t\theta_3\exp(-\theta_1t)
$$

$$
\frac{\partial\eta(t, \theta)}{\partial\theta_2} = -t\theta_3\exp(-\theta_2t)
$$

$$
\frac{\partial\eta(t, \theta)}{\partial\theta_3} = \exp(-\theta_2t) - \exp(-\theta_1t)
$$

We can implement this model using a custom gradient function.

```{r}
grad.compartmental = function(x, theta) {
  
  g1 = x * theta[3] * exp(-theta[1] * x)
  g2 = -x * theta[3] * exp(-theta[2] * x)
  g3 = exp(-theta[2] * x) - exp(-theta[1] * x)
  return(c(g1, g2, g3))
}
```

The compartmental model was used to analyze data on the concentration of theophylline in the blood of a horse. The parameter estimates obtained were $\theta_1 = 4.29$, $\theta_2 = 0.0589$, $\theta_3 = 21.80$. Time is measured in minutes. Let's plot against the original data.

```{r}
library(dplyr)
library(ggplot2)

theta = c(4.29, 0.0589, 21.80)
ex1.5_dat = data.frame(
  t = c(0.166, 0.33, 0.5, 0.666, 1, 1.5, 2, 2.5, 3, 4, 5, 6, 8, 10, 12, 24, 30, 48),
  conc = c(10.1, 14.8, 19.9, 22.1, 20.8, 20.3, 19.7, 18.9, 17.3, 16.1, 15.0, 14.2,
           13.2, 12.3, 10.8, 6.5, 4.6, 1.7)
) %>%
  mutate(
    y = theta[3]*(exp(-theta[2]*t) - exp(-theta[1]*t))
  )

ggplot(ex1.5_dat, aes(x = t, y = conc)) +
  geom_point() +
  geom_line(aes(x = t, y=y)) + 
  scale_x_log10() +
  theme_bw()
```

Let's find the locally D-optimal design.

```{r}
out_D = nlodm(
  model = NULL,
  grad_fun = grad.compartmental,
  obj = 'D',
  theta = theta,
  bound = max(ex1.5_dat$t),
  pts = 3,
  algorithm = 'DE',
  iter = 300,
  swarm = 30,
  seed = 1048
)
```

```{r}
out_D$design
```

```{r}
out_D$plot + scale_x_log10() + labs(x = "log10(time)")
```

The optimal design is to take equal numbers of samples at t=0.23, 1.39, and 18.40 minutes.

Now let's find a few c-optimal designs to estimate interesting quantities. First, the area under the curve (AUC). For this model, the AUC is

$$
\text{AUC} = \int_0^\infty \eta(t, \theta)dt = \theta_3\left(\frac{1}{\theta_2} - \frac{1}{\theta_1}\right) = g_1(\theta).
$$

We are also interesting the time to maximum concentration, which is

$$
t_{max} = \frac{\log\theta_1-\log\theta_2}{\theta_1 - \theta_2} = g_2(\theta).
$$

Finally, we are interested in the maximum concentration

$$
\eta(t_{max}, \theta) = \dots = g_3(\theta)
$$

To find the c-vector for each of these problems, we can take the gradient of the $g$ function. We will skip this step and use the results from p263 of Atkinson.

A major issue with these c-optimal designs (and c-optimal designs in general) is that they have fewer points than what is needed to fit the model. As a work around, Atkinson uses the criterion $$
\Psi_{c}^\epsilon(M) = c^T(M + \epsilon I)^{-1}c
$$ where $\epsilon$ is some small positive value, such as 1E-5. This solves the issue of $M$ not being invertible. This criterion is available using the `'c_e'` option.

```{r}
out_auc = nlodm(
  grad_fun = grad.compartmental,
  obj = 'c_e',
  theta = theta,
  bound = max(ex1.5_dat$t),
  pts = 2,
  algorithm = 'DE',
  iter = 500,
  swarm = 30,
  seed = 1108,
  c = c(theta[3]/theta[1]^2, -theta[3]/theta[2]^2, 1/theta[2]-1/theta[1])
)
```

```{r}
out_auc$plot
```

```{r}
out_auc$design
```

```{r}
a = theta[1] - theta[2]
b = log(theta[1]/theta[2])
out_tmax = nlodm(
  grad_fun = grad.compartmental,
  obj = 'c_e',
  theta = theta,
  bound = max(ex1.5_dat$t),
  pts = 2,
  algorithm = 'DE',
  iter = 500,
  swarm = 30,
  seed = 1118,
  c = c((a/theta[1] - b)/a^2, (b-a/theta[2])/a^2, 0)
)
```

```{r}
out_tmax$plot
```

```{r}
out_tmax$design
```

```{r}
tmax = b/a
e1 = exp(-theta[1]*tmax)
e2 = exp(-theta[2]*tmax)
f = theta[1]*e1 - theta[2]*e2
out_ymax = nlodm(
  grad_fun = grad.compartmental,
  obj = 'c_e',
  theta = theta,
  bound = max(ex1.5_dat$t),
  pts = 1,
  algorithm = 'DE',
  iter = 500,
  swarm = 30,
  seed = 1121,
  c = c(theta[3]*(e1*tmax+f*((a/theta[1] - b)/a^2)),
        theta[3]*(-e2*tmax + f*((b-a/theta[2])/a^2)),
        e2 - e1)
)
```

```{r}
out_ymax$plot
```

```{r}
out_ymax$design
```

These designs all have fewer than 3 design points, which means we had to result to using the modified c-criterion. Let's try the Bayesian designs suggested later in the book in section 18.4.

Set up prior on $\theta_1$ and $\theta_2$, this is prior II from Atkinson that keeps the requirement that $\theta_1 > \theta_2$. Note that the book flips $\theta_1$ and $\theta_2$ in this chapter for some reason.

```{r}
set.seed(326)
N = 10
theta2s = runif(N, .05884-.01, .05884+.01)
theta1s = runif(N, theta2s, 4.298 + 4.0)
theta3s = rep(21.8, N)
thetam = matrix(c(theta1s, theta2s, theta3s), nrow = N)
```

Plot prior distribution of curves.

```{r}
library(tidyr)
thetam %>%
  cbind(matrix(rep(ex1.5_dat$t, N), nrow=N, byrow = T)) %>%
  as.data.frame() %>%
  mutate(sample = 1:N) %>%
  rename(
    theta1 = V1,
    theta2 = V2,
    theta3 = V3
  ) %>%
  pivot_longer(cols = starts_with("V"), values_to = 'time') %>%
  mutate(
    y = theta3*(exp(-theta2*time) - exp(-theta1*time))
  ) %>%
  ggplot(aes(x = time, y = y, color = as.factor(sample))) +
  geom_point() + geom_line() + theme_bw() +
  scale_x_log10()
  
```

Now find the Bayesian designs using these prior values. Starting with Bayesian D-optimal.

```{r}
out_D_bayes = nlodm(
  model = NULL,
  grad_fun = grad.compartmental,
  obj = 'D',
  theta = thetam,
  bound = max(ex1.5_dat$t),
  pts = 6,
  algorithm = 'DE',
  iter = 1000,
  swarm = 50,
  seed = 1048,
  prior_weights = rep(1/N, N)
)
```

```{r}
out_D_bayes$plot
```

```{r}
out_D_bayes$design
```

```{r}
out_auc_bayes = nlodm(
  grad_fun = grad.compartmental,
  obj = 'c',
  theta = thetam,
  bound = max(ex1.5_dat$t),
  pts = 3,
  algorithm = 'DE',
  iter = 1000,
  swarm = 50,
  seed = 409,
  c = cbind(thetam[,3]/thetam[,1]^2, -thetam[,3]/thetam[,2]^2, 1/thetam[,2]-1/thetam[,1]),
  prior_weights = rep(1/N, N)
)
```

```{r}
out_auc_bayes$plot
```

```{r}
out_auc_bayes$design
```
```{r}
a = thetam[,1] - thetam[,2]
b = log(thetam[,1]/thetam[,2])
out_tmax_bayes = nlodm(
  grad_fun = grad.compartmental,
  obj = 'c',
  theta = thetam,
  bound = max(ex1.5_dat$t),
  pts = 3,
  algorithm = 'DE',
  iter = 500,
  swarm = 50,
  seed = 1118,
  c = cbind((a/thetam[,1] - b)/a^2, (b-a/thetam[,2])/a^2, rep(0, nrow(thetam)))
)
```

```{r}
out_tmax_bayes$plot
```
```{r}
out_tmax_bayes$design
```
The optimal design is likely still singular.

```{r}
tmax = b/a
e1 = exp(-thetam[,1]*tmax)
e2 = exp(-thetam[,2]*tmax)
f = thetam[,1]*e1 - thetam[,2]*e2
out_ymax_bayes = nlodm(
  grad_fun = grad.compartmental,
  obj = 'c',
  theta = thetam,
  bound = max(ex1.5_dat$t),
  pts = 4,
  algorithm = 'DE',
  iter = 500,
  swarm = 30,
  seed = 1121,
  c = cbind(thetam[,3]*(e1*tmax+f*((a/thetam[,1] - b)/a^2)),
        thetam[,3]*(-e2*tmax + f*((b-a/thetam[,2])/a^2)),
        e2 - e1)
)
```

```{r}
out_ymax_bayes$plot
```
```{r}
out_ymax_bayes$design
```
Same story with this Bayesian design. The optimal design is likely singular.
